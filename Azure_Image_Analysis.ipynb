{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJf74PRJPwcN"
      },
      "source": [
        "## Azure Image Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-cognitiveservices-vision-computervision\n",
        "!pip install azure-storage-blob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ZMIlGSTVB9",
        "outputId": "728aa3e7-3698-4591-f8c6-caf099c8857a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: azure-cognitiveservices-vision-computervision in /usr/local/lib/python3.8/dist-packages (0.9.0)\n",
            "Requirement already satisfied: msrest>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from azure-cognitiveservices-vision-computervision) (0.7.1)\n",
            "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.8/dist-packages (from azure-cognitiveservices-vision-computervision) (1.1.28)\n",
            "Requirement already satisfied: requests~=2.16 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.25.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.3.1)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (0.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2022.12.7)\n",
            "Requirement already satisfied: azure-core>=1.24.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.26.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.26.14)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.8/dist-packages (12.15.0)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.26.0 in /usr/local/lib/python3.8/dist-packages (from azure-storage-blob) (1.26.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from azure-storage-blob) (4.5.0)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from azure-storage-blob) (0.6.1)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from azure-storage-blob) (39.0.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from azure-core<2.0.0,>=1.26.0->azure-storage-blob) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.8/dist-packages (from azure-core<2.0.0,>=1.26.0->azure-storage-blob) (2.25.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.26.0->azure-storage-blob) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.26.0->azure-storage-blob) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.26.0->azure-storage-blob) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.26.0->azure-storage-blob) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW-M05GLPwcP"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "from array import array\n",
        "import os\n",
        "from PIL import Image\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxXn-Z7qPwcQ"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn96i8uuPwcR"
      },
      "source": [
        "**The following image variables will be analyzed during the image analysis:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IakicNvgPwcR"
      },
      "source": [
        "1. Presence of text\n",
        "2. People-centered vs. text-centered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcP75CZHPwcR"
      },
      "source": [
        "For 1. Presence of text, there are two approaches to extract this variable:\n",
        "\n",
        "(1) Use OCR (Optical Character Recognition) to extract the text content of each image. \n",
        "\n",
        "(2) Use image tag to get the confidence score (likelyhood, or probabilty) of text portion, if the text portion is greater than a certain thereshold (e.g., > 0.05 since we only care about the presence of text),  consider the image as containing text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6lFjGjzPwcR"
      },
      "source": [
        "For 2. People-centered vs. text-centered, there are three approaches to extract this variable:\n",
        "\n",
        "(1) Use image tag to get the confidence score (likelyhood, or probabililty) of text AND people portion when both text and people tags are detected, assign the image as text/people centered with the higher confidence score on one type\n",
        "\n",
        "(2) Use image description (add captions to images), a sentence will be generated, find if the image is text / people centered.\n",
        "\n",
        "(3) Use image category. Unlike tags, categories are organized in a parent/child hierarchy, and there are fewer of them (86, as opposed to thousands of tags). Find if the image is text / people centered by checking if the category is \"people_\" or \"text_\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVYfgzWJPwcS"
      },
      "source": [
        "**Note:** \n",
        "\n",
        "The Azure accepts local image files or image URL links (e.g.: https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/cognitive-services/Computer-vision/Images/readsample.jpg) for the analysis. \n",
        "\n",
        "Image URL under the Google drive is not valid (Bad Request). An alternative is to upload images to the Azure Storage Account\n",
        "\n",
        "Right now use the image URL generated by Azure Storage Account for the demonstration purpose. \n",
        "\n",
        "TBD: Whether to analyze through local paths or URLs\n",
        "\n",
        "The image file size should be less than 4MB, may need to compress some images\n",
        "\n",
        "Update: The Instagram seems to have compressed the images already before saving them into its database, so each image should not exceed the 4MB file size limit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN7FXwdUPwcS"
      },
      "source": [
        "## Extracting Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JniW3ZPwcS"
      },
      "outputs": [],
      "source": [
        "# Keys and endpoint from Microsoft Azure\n",
        "# Note: Not valid anymore\n",
        "subscription_key = \"32f61e402816420fb0b173d80fb7df00\"\n",
        "endpoint = \"https://aijins-computer-vision.cognitiveservices.azure.com/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv-liiAZPwcT"
      },
      "outputs": [],
      "source": [
        "# Authentication\n",
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save images to bolb\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "connect_str = 'DefaultEndpointsProtocol=https;AccountName=aijinsimagedata;AccountKey=RmcEhNQ4LCthPRo22NgQXtq5eDu2oE8AnnT3nprzmAnH8TM7O5I+deG2PJzpEPsn4CpehXlGFaek+AStqiwsmA==;EndpointSuffix=core.windows.net'\n",
        "container_name = 'newimage'\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "container_client = blob_service_client.get_container_client(container_name)"
      ],
      "metadata": {
        "id": "rAyZGGz3e0Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_urls = []\n",
        "blobs = container_client.list_blobs()\n",
        "for blob in blobs:\n",
        "    image_url = f\"https://aijinsimagedata.blob.core.windows.net/{container_name}/{blob.name}\"\n",
        "    image_urls.append(image_url)\n",
        "\n",
        "url = \"https://insimagestorage.blob.core.windows.net/image/profile_post_img_all/freeofviolence/269812685_772090997519875_1762577499023572044_n.jpg\""
      ],
      "metadata": {
        "id": "Bv5SC4dlKIfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1obcmol64aw",
        "outputId": "07221a46-897b-4891-a777-c1fb938736c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14618"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umeRfzyrPwcT"
      },
      "source": [
        "For local image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eoNpTZlPwcT"
      },
      "outputs": [],
      "source": [
        "# #images_folder = os.path.join (os.path.dirname(os.path.abspath('C:\\Users\\shiho\\Aij\\SA_Instagram')), \"Images\")\n",
        "\n",
        "# images_folder = \"C:/Users/shiho/Aij/SA_Instagram/Images/\"\n",
        "# local_image_path = os.path.join (images_folder, \"199142856-986c1f8c-10f9-4973-91a1-573d927ec7bf.jpg\")\n",
        "# local_image = open(local_image_path, \"rb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-FyNGp1PwcT"
      },
      "source": [
        "For Azure Storage Account:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIsjhQ12PwcT"
      },
      "outputs": [],
      "source": [
        "image_folder_azure = \"https://insimagestorage.blob.core.windows.net/image/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2oaPpWKPwcT"
      },
      "outputs": [],
      "source": [
        "# people with text ()\n",
        "image_name = \"199142809-bb24a9ff-ebb7-476b-90ee-dc7f4dd0e172.jpg\"\n",
        "# image without text\n",
        "#image_name = \"289842202_737546634131529_4184733584509506915_n.jpg\"\n",
        "\n",
        "#image_name = \"292969378_1193562068106318_3930416080560697894_n.webp\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXYqGz4SPwcU",
        "outputId": "9174618b-14e7-4fb2-f578-578bcd0be1db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://insimagestorage.blob.core.windows.net/image/199142809-bb24a9ff-ebb7-476b-90ee-dc7f4dd0e172.jpg'"
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "read_image_url = image_folder_azure + image_name\n",
        "read_image_url"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For unzip files:"
      ],
      "metadata": {
        "id": "daeCN9HqSp_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "url = 'https://aijinsimagedata.blob.core.windows.net/newimage/profile_post_img_all.zip'\n",
        "filename = 'profile_post_img_all.zip'\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('images')"
      ],
      "metadata": {
        "id": "oBlKfCdsSuRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = []\n",
        "for dirpath, dirnames, filenames in os.walk('images/profile_post_img_all'):\n",
        "    for filename in filenames:\n",
        "        # Build image URL\n",
        "        image_path = os.path.join(dirpath, filename)\n",
        "\n",
        "        with open(image_path, 'rb') as data:\n",
        "              container_client.upload_blob(name=image_path, data=data)\n",
        "              image_url.append(\"https://aijinsimagedata.blob.core.windows.net/newimage/\"+image_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "SLJvs8QqXKLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VusiMkW-4L_E",
        "outputId": "2c430bd0-828a-41b9-e586-1a4cd4bd05d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53071"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0SU6S-qPwcU"
      },
      "source": [
        "### Presence of Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE3byn7vPwcU"
      },
      "source": [
        "#### (1) Use OCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-CNPhclPwcU"
      },
      "source": [
        "Demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJtr6nhXPwcU"
      },
      "outputs": [],
      "source": [
        "# Call API with URL and raw response (allows you to get the operation location)\n",
        "read_response = computervision_client.read(image_url[0],  raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLPVa_3mSdMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij11alJ3PwcU"
      },
      "outputs": [],
      "source": [
        "# Get the operation location (URL with an ID at the end) from the response\n",
        "read_operation_location = read_response.headers[\"Operation-Location\"]\n",
        "\n",
        "# Grab the ID from the URL\n",
        "operation_id = read_operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Call the \"GET\" API and wait for it to retrieve the results \n",
        "while True:\n",
        "    read_result = computervision_client.get_read_result(operation_id)\n",
        "    if read_result.status not in ['notStarted', 'running']:\n",
        "        break\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtbQxFiuPwcV"
      },
      "outputs": [],
      "source": [
        "# Print the detected text, line by line\n",
        "if read_result.status == OperationStatusCodes.succeeded:\n",
        "    for text_result in read_result.analyze_result.read_results:\n",
        "        line_text = []\n",
        "        line_bouding_box = []\n",
        "        for line in text_result.lines:\n",
        "            line_text.append(line.text)\n",
        "            line_bouding_box.append(line.bounding_box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghcqnmnIPwcV",
        "outputId": "b0dc008f-cfdd-4d14-d3ac-8f0c1d6f8649"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['G13',\n",
              " 'Concertation',\n",
              " 'nationale',\n",
              " 'féministe',\n",
              " 'MINISTÈRE DES DROITS',\n",
              " \"DES FEMMES ET DE L'ÉGALITÉ\",\n",
              " 'Pour plus',\n",
              " 'de stabilité',\n",
              " 'Avoir un ministère des Droits des',\n",
              " \"femmes et de l'Égalité permettrait\",\n",
              " \"d'assurer une plus grande\",\n",
              " \"stabilité dans l'administration des\",\n",
              " 'fonds et dans la mise en œuvre',\n",
              " 'des politiques.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "line_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z1XTi7XPwcV",
        "outputId": "9d50830f-1c7c-4e5c-dc1e-4e352667e156"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[819.0, 64.0, 921.0, 62.0, 924.0, 111.0, 819.0, 115.0],\n",
              " [928.0, 63.0, 1020.0, 63.0, 1020.0, 79.0, 928.0, 79.0],\n",
              " [925.0, 77.0, 991.0, 79.0, 990.0, 96.0, 925.0, 95.0],\n",
              " [926.0, 94.0, 990.0, 97.0, 989.0, 112.0, 926.0, 109.0],\n",
              " [826.0, 133.0, 971.0, 133.0, 971.0, 145.0, 826.0, 145.0],\n",
              " [826.0, 149.0, 1010.0, 148.0, 1011.0, 160.0, 826.0, 162.0],\n",
              " [569.0, 520.0, 907.0, 520.0, 907.0, 586.0, 569.0, 586.0],\n",
              " [571.0, 591.0, 972.0, 591.0, 972.0, 654.0, 571.0, 654.0],\n",
              " [451.0, 738.0, 999.0, 738.0, 999.0, 772.0, 451.0, 772.0],\n",
              " [451.0, 777.0, 1013.0, 778.0, 1013.0, 819.0, 451.0, 817.0],\n",
              " [451.0, 821.0, 883.0, 824.0, 883.0, 861.0, 451.0, 858.0],\n",
              " [449.0, 863.0, 1004.0, 863.0, 1004.0, 902.0, 449.0, 901.0],\n",
              " [449.0, 906.0, 971.0, 908.0, 971.0, 944.0, 449.0, 942.0],\n",
              " [450.0, 949.0, 688.0, 950.0, 688.0, 989.0, 450.0, 987.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "line_bouding_box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkUIwRr4PwcV"
      },
      "source": [
        "Write Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPS9HCIHPwcV"
      },
      "outputs": [],
      "source": [
        "def azure_ocr(read_image_url):\n",
        "    '''Use Azure Computer Vision OCR to extract text from an image\n",
        "    Return two variables, all in list format\n",
        "    line_text is the actural text extracted\n",
        "    line_bouding_box is the region of the text in the image\n",
        "    '''\n",
        "    # Call API with URL and raw response (allows you to get the operation location)\n",
        "    read_response = computervision_client.read(read_image_url, raw=True)\n",
        "    \n",
        "    # Get the operation location (URL with an ID at the end) from the response\n",
        "    read_operation_location = read_response.headers[\"Operation-Location\"]\n",
        "    \n",
        "    # Grab the ID from the URL\n",
        "    operation_id = read_operation_location.split(\"/\")[-1]\n",
        "    \n",
        "    # Call the \"GET\" API and wait for it to retrieve the results\n",
        "    while True:\n",
        "        read_result = computervision_client.get_read_result(operation_id)\n",
        "        if read_result.status not in ['notStarted', 'running']:\n",
        "            break\n",
        "        time.sleep(1)\n",
        "    # Print the detected text, line by line\n",
        "    if read_result.status == OperationStatusCodes.succeeded:\n",
        "        for text_result in read_result.analyze_result.read_results:\n",
        "            line_text = []\n",
        "            line_bouding_box = []\n",
        "            for line in text_result.lines:\n",
        "                line_text.append(line.text)\n",
        "                line_bouding_box.append(line.bounding_box)\n",
        "    return line_text, line_bouding_box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gec274f-PwcV",
        "outputId": "a0369d3a-41a7-43d8-c454-82fb8b332c48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['G13',\n",
              " 'Concertation',\n",
              " 'nationale',\n",
              " 'féministe',\n",
              " 'MINISTÈRE DES DROITS',\n",
              " \"DES FEMMES ET DE L'ÉGALITÉ\",\n",
              " 'Pour plus',\n",
              " 'de stabilité',\n",
              " 'Avoir un ministère des Droits des',\n",
              " \"femmes et de l'Égalité permettrait\",\n",
              " \"d'assurer une plus grande\",\n",
              " \"stabilité dans l'administration des\",\n",
              " 'fonds et dans la mise en œuvre',\n",
              " 'des politiques.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "line_text, line_bouding_box = azure_ocr(image_url[0])\n",
        "line_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SFEHX2dPwcV",
        "outputId": "3b75705b-18d9-4b52-855e-c6a8af363dba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOEcVOb9PwcV"
      },
      "outputs": [],
      "source": [
        "def text_presence_by_ocr(line_text = line_text):\n",
        "    '''Identify if the image contains the text through image OCR approach\n",
        "    The text is present if the line_text is empty\n",
        "    Return boolean True or False\n",
        "    '''\n",
        "    if len(line_text) != 0:\n",
        "        text_presence = True\n",
        "    else:\n",
        "        text_presence = False\n",
        "    return text_presence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO2S8YVbPwcV",
        "outputId": "cefea330-e747-494e-c227-a2bacc5a2e62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "has_text_ocr = text_presence_by_ocr(line_text = line_text)\n",
        "has_text_ocr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfco0shKPwcV"
      },
      "source": [
        "**(2) Use Image Tag**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVsA_Wc9PwcV"
      },
      "source": [
        "Demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENwfcWydPwcW"
      },
      "outputs": [],
      "source": [
        "tags_result = computervision_client.tag_image(image_url[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "las6akuQPwcW",
        "outputId": "1335ca92-4832-4c27-dd23-01fd555fa4b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag at 0x7f3879420160>,\n",
              " <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag at 0x7f38793bf5e0>,\n",
              " <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag at 0x7f38793bfd90>,\n",
              " <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag at 0x7f38793bfeb0>,\n",
              " <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag at 0x7f3877847c70>]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "tags_result.tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-_2StIWPwcW"
      },
      "outputs": [],
      "source": [
        "if (len(tags_result.tags) == 0):\n",
        "    print(\"No tags detected.\")\n",
        "else:\n",
        "    tag_name = []\n",
        "    tag_confidence = []\n",
        "    for tag in tags_result.tags:\n",
        "        tag_name.append(tag.name)\n",
        "        tag_confidence.append(tag.confidence)\n",
        "        #print(\"'{}' with confidence {:.2f}%\".format(tag.name, tag.confidence * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvDWfemcPwcW",
        "outputId": "67f3cfb5-048b-4e30-ef83-1a759790d9e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text', 'poster', 'graphic design', 'megaphone', 'design']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "tag_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0YR3RIBPwcW",
        "outputId": "fb9ec702-2b66-405a-ac0f-c999c0d7d7cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9998866319656372,\n",
              " 0.9135843515396118,\n",
              " 0.8816847801208496,\n",
              " 0.8532842397689819,\n",
              " 0.5847171545028687]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "tag_confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf92L5ZaPwcW",
        "outputId": "49a477c2-f806-430f-a7f5-ca06a7039ac2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 0.9998866319656372,\n",
              " 'poster': 0.9135843515396118,\n",
              " 'graphic design': 0.8816847801208496,\n",
              " 'megaphone': 0.8532842397689819,\n",
              " 'design': 0.5847171545028687}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# combine the two list into dictionary\n",
        "res = {tag_name[i]: tag_confidence[i] for i in range(len(tag_name))}\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOWrkyfcPwcW",
        "outputId": "c9517342-f81a-49e7-c122-c41fd72366ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there is text\n"
          ]
        }
      ],
      "source": [
        "# Detect if text is in the image\n",
        "if \"text\" in res:\n",
        "    print(\"there is text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "740SIhlfPwcW"
      },
      "source": [
        "Write Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8Ae8cEhPwcW"
      },
      "outputs": [],
      "source": [
        "def azure_image_tag(read_image_url):\n",
        "    '''Use Azure Computer Vision Image Tag to assign tags of an image\n",
        "    Return a variable in dictionary format\n",
        "    The key of the dictionary is the name of the image tag\n",
        "    The value of the dictionary is the confidence score of corresponding image tag\n",
        "    '''\n",
        "    tags_result = computervision_client.tag_image(read_image_url)\n",
        "    \n",
        "    # Return empty dictionary if there is no image tag\n",
        "    if (len(tags_result.tags) == 0):\n",
        "        image_tags = {}\n",
        "        \n",
        "    # Get all the image tags with its confidence\n",
        "    else:\n",
        "        tag_name = []\n",
        "        tag_confidence = []\n",
        "        for tag in tags_result.tags:\n",
        "            tag_name.append(tag.name)\n",
        "            tag_confidence.append(tag.confidence)\n",
        "        image_tags = {tag_name[i]: tag_confidence[i] for i in range(len(tag_name))}\n",
        "    return image_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUxOdN-_PwcW",
        "outputId": "b765ef31-50b1-4298-b9be-9d822da8c92a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 0.9998866319656372,\n",
              " 'poster': 0.9135843515396118,\n",
              " 'graphic design': 0.8816847801208496,\n",
              " 'megaphone': 0.8532842397689819,\n",
              " 'design': 0.5847171545028687}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "image_tags = azure_image_tag(image_url[0])\n",
        "image_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyz-bJqTPwcW"
      },
      "outputs": [],
      "source": [
        "def text_presence_by_tag(image_tags = image_tags):\n",
        "    '''Identify if the image contains the text through image tag approach\n",
        "    Return boolean True or False\n",
        "    '''\n",
        "    if \"text\" in image_tags:\n",
        "        text_presence = True\n",
        "    else:\n",
        "        text_presence = False\n",
        "    return text_presence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVgSeZyWPwcW",
        "outputId": "077c588e-6057-4b67-ca0e-dadea4ddbdbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "has_text_tag = text_presence_by_tag(image_tags)\n",
        "has_text_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWTwFduePwcX"
      },
      "source": [
        "### People-centered vs. Text-centered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60EU5-gnPwcX"
      },
      "source": [
        "**(1) Use Image Tag**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lajv5WlPwcX"
      },
      "source": [
        "May need to run more people related tags to find the human-related tag categories.\n",
        "\n",
        "Or use network analysis to find the tag relations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc1qjXN8PwcX"
      },
      "source": [
        "**Right now write a function to output a list of unique tags for identifying people-centered tags**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAo8vRTuPwcX"
      },
      "outputs": [],
      "source": [
        "def find_unique_tag(image_tags = image_tags):\n",
        "    unique_tags = list(image_tags)\n",
        "    return unique_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsOY8jBFPwcX",
        "outputId": "0316a97a-bc5a-4e70-b594-84f73cacb656"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text', 'poster', 'graphic design', 'megaphone', 'design']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "unique_tags = find_unique_tag(image_tags=image_tags)\n",
        "unique_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-8ybVAAPwcX",
        "outputId": "1267adbf-b270-49c6-e3a3-c904156e2092"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "[k for k in res if re.match('human', k)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMIg58HzPwcX",
        "outputId": "9ac0bd1f-347c-4aab-8546-04cbfe988b6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "\"human face\" in res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "X3jiAfaXPwcX",
        "outputId": "e19bffef-916f-4789-ed61-30c7382b9078"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-81b600684d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mocr_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ocr_result' is not defined"
          ]
        }
      ],
      "source": [
        "str(ocr_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6dyr3WYlPwcX",
        "outputId": "bb6dda8b-8a5f-42b0-eb4c-62ab01e2fbb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<azure.cognitiveservices.vision.computervision.models._models_py3.OcrRegion at 0x1deea83d910>,\n",
              " <azure.cognitiveservices.vision.computervision.models._models_py3.OcrRegion at 0x1deea83de50>,\n",
              " <azure.cognitiveservices.vision.computervision.models._models_py3.OcrRegion at 0x1deea2103a0>,\n",
              " <azure.cognitiveservices.vision.computervision.models._models_py3.OcrRegion at 0x1deea210460>]"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ocr_result.regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znxKd3kLPwcX"
      },
      "source": [
        "**(2) User Image Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Qi49IDPwcY"
      },
      "source": [
        "Demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsyhX64JPwcY"
      },
      "outputs": [],
      "source": [
        "# Call API\n",
        "description_result = computervision_client.describe_image(read_image_url,language= 'en' , max_candidates=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lru8ho5rPwcY",
        "outputId": "c786949c-0cc2-4a4d-e8e4-c1eff7752a9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'additional_properties': {}, 'tags': ['text', 'woman', 'person', 'screenshot', 'businesscard'], 'captions': [<azure.cognitiveservices.vision.computervision.models._models_py3.ImageCaption object at 0x000001DEE9DCCEB0>], 'request_id': 'bd02b7e9-c744-4c71-9b75-9d862da1ed0f', 'metadata': <azure.cognitiveservices.vision.computervision.models._models_py3.ImageMetadata object at 0x000001DEEA823370>, 'model_version': '2021-05-01'}\""
            ]
          },
          "execution_count": 223,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(description_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPkZJ315PwcY"
      },
      "outputs": [],
      "source": [
        "if (len(description_result.captions) == 0):\n",
        "    description_text = []\n",
        "    description_confidence = []\n",
        "else:\n",
        "    description_text = []\n",
        "    description_confidence = []\n",
        "    for caption in description_result.captions:\n",
        "        description_text.append(caption.text)\n",
        "        description_confidence.append(caption.confidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vsf2fn0PwcY",
        "outputId": "7bef5a53-4315-4dea-b9d9-80c774917b8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a woman holding a box']"
            ]
          },
          "execution_count": 225,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmnCKyEsPwcY",
        "outputId": "d0894343-b201-4373-85ce-81e79d29e53f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5649126768112183]"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description_confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPLNjfQmPwcY"
      },
      "outputs": [],
      "source": [
        "def azure_image_description(read_image_url):\n",
        "    '''Use Azure Computer Vision Image Description to describe an image\n",
        "    Return two variables in list format\n",
        "    The description_text is the text description of the image\n",
        "    The description_confidence is the confidence score of the description\n",
        "    '''\n",
        "    description_result = computervision_client.describe_image(read_image_url,language= 'en' , max_candidates=3)\n",
        "    if (len(description_result.captions) == 0):\n",
        "        description_text = []\n",
        "        description_confidence = []\n",
        "    else:\n",
        "        description_text = []\n",
        "        description_confidence = []\n",
        "        for caption in description_result.captions:\n",
        "            caption_text = caption.text\n",
        "            description_text.append(caption_text)\n",
        "            description_confidence.append(caption.confidence)\n",
        "    return description_text, description_confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1cpMFnGPwcY"
      },
      "outputs": [],
      "source": [
        "description_text, description_confidence = azure_image_description(read_image_url=read_image_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i934sZtqPwcY",
        "outputId": "e4a7fd59-4eb3-488b-a2a4-506444ad8eac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a woman holding a box']"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJKPbjYpPwcY"
      },
      "source": [
        "**(3) Use Image Catogory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q80etH74PwcY"
      },
      "source": [
        "Demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVHpa7xXPwcY"
      },
      "outputs": [],
      "source": [
        "analyze_result = computervision_client.analyze_image(read_image_url, visual_features= ['Categories'], language= 'en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQjBBodTPwcY",
        "outputId": "a499ccbb-44f8-4898-dbaf-e1eb4122f63c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'additional_properties': {}, 'categories': [<azure.cognitiveservices.vision.computervision.models._models_py3.Category object at 0x000001DEEF2EA7F0>], 'adult': None, 'color': None, 'image_type': None, 'tags': None, 'description': None, 'faces': None, 'objects': None, 'brands': None, 'request_id': '6eb93eed-cc8e-47d9-9474-a57557dce63a', 'metadata': <azure.cognitiveservices.vision.computervision.models._models_py3.ImageMetadata object at 0x000001DEEF44EB50>, 'model_version': '2021-05-01'}\""
            ]
          },
          "execution_count": 287,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(analyze_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usgpXHnEPwcY",
        "outputId": "33fb550e-0d97-40db-bf18-6cde60209efc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['text_mag']"
            ]
          },
          "execution_count": 289,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for category in analyze_result.categories:\n",
        "    category_name = []\n",
        "    category_score = []\n",
        "    category_name.append(category.name)\n",
        "    category_score.append(category.score)\n",
        "category_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiSiSgY3PwcZ"
      },
      "source": [
        "Write Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6j54FVVPwcZ"
      },
      "outputs": [],
      "source": [
        "def azure_image_category(read_image_url):\n",
        "    '''Use Azure Computer Vision Analyze Image to categorize an image\n",
        "    Return two variables in list format\n",
        "    The category_name is the category of the image\n",
        "    The category_score is the confidence score of corresponding category\n",
        "    '''\n",
        "    analyze_result = computervision_client.analyze_image(read_image_url, visual_features= ['Categories'], language= 'en')\n",
        "    if (len(analyze_result.categories) == 0):\n",
        "        category_name = []\n",
        "        category_score = []\n",
        "    else:\n",
        "        category_name = []\n",
        "        category_score = []\n",
        "        for category in analyze_result.categories:\n",
        "            category_name.append(category.name)\n",
        "            category_score.append(category.score)\n",
        "    return category_name, category_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_14Jnuo6PwcZ"
      },
      "outputs": [],
      "source": [
        "category_name, category_score = azure_image_category(read_image_url=read_image_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxdZwhOkPwcZ",
        "outputId": "0b567fd3-b0b7-47ee-a7c0-63720150449c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['text_mag']"
            ]
          },
          "execution_count": 293,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "category_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBCcZvG0PwcZ",
        "outputId": "4a719643-45d6-4e8b-eb49-0a313e5019f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.890625]"
            ]
          },
          "execution_count": 294,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "category_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9cJNqanPwcZ"
      },
      "source": [
        "## Demo for creating dataset for image analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "escLajRMPwcZ"
      },
      "source": [
        "The desired output is a dataframe with image URL, image name, and variables in interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWcXR9uiPwcZ"
      },
      "source": [
        "A list of images in Azure Storage Account:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErYC8kJYPwcZ"
      },
      "outputs": [],
      "source": [
        "# TODO: This list of all files under Azure could be obtained through Azure Data Factory\n",
        "image_folder_item = ['199142760-01f4f00a-a46f-4ea8-a7aa-02eb76a35a33.jpg',\n",
        "                    '199142769-b6cdb0aa-f267-4570-ad0d-0663703c7974.jpg',\n",
        "                    '199142809-bb24a9ff-ebb7-476b-90ee-dc7f4dd0e172.jpg',\n",
        "                    '199142848-e0cd2641-0b01-40e9-be3c-bcb73ba1c635.jpg',\n",
        "                    '199142856-986c1f8c-10f9-4973-91a1-573d927ec7bf.jpg',\n",
        "                    '289842202_737546634131529_4184733584509506915_n.jpg',\n",
        "                    '292969378_1193562068106318_3930416080560697894_n.webp',\n",
        "                    '58409537_459823631424204_7100044955669757952_n.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okQVaY_LPwcZ",
        "outputId": "e84cffe5-4871-46eb-e6d8-b7fccd69e7ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://insimagestorage.blob.core.windows.net/image/'"
            ]
          },
          "execution_count": 245,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "azure_storage_path = \"https://insimagestorage.blob.core.windows.net/image/\"\n",
        "azure_storage_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_url[0].split(\"/\")[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fbAU3b4hpGSU",
        "outputId": "32511aa7-0610-4c82-ef00-3adb62d72e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'306693307_613394816895124_8968443589920319007_n.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bAX9gjxPwcZ"
      },
      "outputs": [],
      "source": [
        "def create_image_analysis_df(image_url):\n",
        "    '''This fuction create a dataframe that contains the image name, URL and the variables in interest\n",
        "    azure_storage_path: Azure Storage Account path\n",
        "    image_folder_item: A list containing all image names. e.g., \"sample_image.jpg\"\n",
        "    Return a pandas dataframe\n",
        "    '''\n",
        "    # First create an empty pandas dataframe for appending each image and its related variable\n",
        "    image_analysis_df = pd.DataFrame(columns= ['image_name', 'image_storage_URL', 'ocr_text', \n",
        "                                               'ocr_text_bounding_box', 'has_text_ocr', 'image_tags', \n",
        "                                               'has_text_tag', 'unique_tags', 'description_text', \n",
        "                                               'description_confidence', 'category_name', 'category_score'])\n",
        "    \n",
        "    for i in range(0, len(image_url)):\n",
        "        # Call all previous functions to extract variables in interest through Azure\n",
        "        read_image_url = image_url[i]\n",
        "        name = read_image_url.split(\"/\")[-1]\n",
        "        # OCR part\n",
        "        line_text, line_bouding_box = azure_ocr(read_image_url = read_image_url)\n",
        "        has_text_ocr = text_presence_by_ocr(line_text = line_text)\n",
        "        \n",
        "        # Image tag part\n",
        "        image_tags = azure_image_tag(read_image_url = read_image_url)\n",
        "        has_text_tag = text_presence_by_tag(image_tags)\n",
        "        unique_tags = find_unique_tag(image_tags=image_tags)\n",
        "        \n",
        "        # Image Description part\n",
        "        description_text, description_confidence = azure_image_description(read_image_url=read_image_url)\n",
        "        \n",
        "        # Image Category part\n",
        "        category_name, category_score = azure_image_category(read_image_url=read_image_url)\n",
        "        \n",
        "        # Put all variables in dictionary format as a row in pd dataframe, append the row\n",
        "        each_image_info = {'image_name':name, 'image_storage_URL': read_image_url, 'ocr_text': line_text, \n",
        "                           'ocr_text_bounding_box': line_bouding_box, 'has_text_ocr': has_text_ocr, 'image_tags': image_tags,\n",
        "                           'has_text_tag': has_text_tag, 'unique_tags': unique_tags, 'description_text': description_text,\n",
        "                           'description_confidence' : description_confidence, 'category_name': category_name, \n",
        "                           'category_score': category_score} \n",
        "        image_analysis_df = image_analysis_df.append(each_image_info, ignore_index= True)\n",
        "        \n",
        "    return image_analysis_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "otiWJuTTPwcZ",
        "outputId": "c1432406-cb3a-423d-f025-94ef7e83c61f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ComputerVisionOcrErrorException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mComputerVisionOcrErrorException\u001b[0m           Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-3f4ab823ff2e>\u001b[0m in \u001b[0;36mcreate_image_analysis_df\u001b[0;34m(image_url)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# OCR part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mline_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_bouding_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mazure_ocr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mhas_text_ocr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_presence_by_ocr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-34b00529371e>\u001b[0m in \u001b[0;36mazure_ocr\u001b[0;34m(read_image_url)\u001b[0m\n\u001b[1;32m      6\u001b[0m     '''\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Call API with URL and raw response (allows you to get the operation location)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mread_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputervision_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get the operation location (URL with an ID at the end) from the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/azure/cognitiveservices/vision/computervision/operations/_computer_vision_client_operations.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, url, language, pages, model_version, reading_order, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m202\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComputerVisionOcrErrorException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mComputerVisionOcrErrorException\u001b[0m: Operation returned an invalid status code 'Bad Request'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "image_analysis_df = create_image_analysis_df(image_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "508J8V78PwcZ",
        "outputId": "1fc0ac8e-9954-4604-dc39-12c317ec7dab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-6b12d0bb131f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_analysis_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'image_analysis_df' is not defined"
          ]
        }
      ],
      "source": [
        "image_analysis_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zpxwzriPwca"
      },
      "outputs": [],
      "source": [
        "image_analysis_df.to_csv('sample_image_analysis_dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-oa0FJcPwca"
      },
      "source": [
        "**Draft Code (for local files)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WWCv0TCPwca",
        "outputId": "b8cbb7d9-cfe8-41d3-f91e-9ef51e5a39bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Description of local image: \n",
            "'a group of people holding a sign' with confidence 66.56%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call API\n",
        "description_result = computervision_client.describe_image_in_stream(local_image,language= 'en' ,max_candidates=3)\n",
        "\n",
        "# Get the captions (descriptions) from the response, with confidence level\n",
        "print(\"Description of local image: \")\n",
        "if (len(description_result.captions) == 0):\n",
        "    print(\"No description detected.\")\n",
        "else:\n",
        "    for caption in description_result.captions:\n",
        "        print(\"'{}' with confidence {:.2f}%\".format(caption.text, caption.confidence * 100))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CwFQWilPwca",
        "outputId": "30933fa4-f935-40e8-a41b-e72a2e8c60bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'a group of people holding a sign'"
            ]
          },
          "execution_count": 236,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for caption in description_result.captions:\n",
        "    caption_text = caption.text\n",
        "    caption_confidence = caption.confidence\n",
        "caption_text  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKvVWt2FPwca",
        "outputId": "c316e2e0-a79a-4d47-d144-0ba7a0ee810f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'additional_properties': {}, 'width': 736, 'height': 945, 'format': 'Jpeg'}\n"
          ]
        }
      ],
      "source": [
        "print(description_result.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfxykGVaPwca"
      },
      "outputs": [],
      "source": [
        "def imageDiscriptiontoDict(description_result):\n",
        "    d = {}\n",
        "    d['additional_properties'] = description_result.additional_properties\n",
        "    d['tags'] = description_result.tags\n",
        "    dcaptions = []\n",
        "    for i in description_result.captions:\n",
        "        dcaptions.append({'additional_properties': i.additional_properties, 'text': i.text, 'confidence': i.confidence})\n",
        "    d['captions'] = dcaptions\n",
        "    d['request_id'] = description_result.request_id\n",
        "    d['metadata'] = {'additional_properties': description_result.metadata.additional_properties, 'width': description_result.metadata.width, 'height': description_result.metadata.height, 'format': description_result.metadata.format}\n",
        "    d['model_version'] = description_result.model_version\n",
        "    \n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-fYQlocPwca",
        "outputId": "490ed1e0-de75-4811-88af-a3abfa88e5c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'additional_properties': {},\n",
              " 'tags': ['text', 'person', 'standing', 'people', 'group'],\n",
              " 'captions': [{'additional_properties': {},\n",
              "   'text': 'a group of people holding a sign',\n",
              "   'confidence': 0.665584146976471}],\n",
              " 'request_id': 'c35173f1-ce86-4252-916d-d798a24c8f5d',\n",
              " 'metadata': {'additional_properties': {},\n",
              "  'width': 736,\n",
              "  'height': 945,\n",
              "  'format': 'Jpeg'},\n",
              " 'model_version': '2021-05-01'}"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imageDiscriptiontoDict(description_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64GPttscPwca",
        "outputId": "864fbae0-a0aa-483e-ad25-133f00bd27fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'additional_properties': {}, 'tags': ['text', 'person', 'standing', 'people', 'group'], 'captions': [<azure.cognitiveservices.vision.computervision.models._models_py3.ImageCaption object at 0x000001DEEA3B4730>], 'request_id': 'c35173f1-ce86-4252-916d-d798a24c8f5d', 'metadata': <azure.cognitiveservices.vision.computervision.models._models_py3.ImageMetadata object at 0x000001DEEA4BA5B0>, 'model_version': '2021-05-01'}\""
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(description_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekh4PtqqPwca",
        "outputId": "b56d828b-5a0c-4b38-d74d-5f7a1f50f30e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'additional_properties': {}, 'text': 'a woman holding a box', 'confidence': 0.5649126768112183}\n"
          ]
        }
      ],
      "source": [
        "print(description_result.captions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4gKUuvdPwca",
        "outputId": "e70f4813-a1bf-4008-bde9-01803b5ea40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method Model.as_dict of <azure.cognitiveservices.vision.computervision.models._models_py3.ImageDescription object at 0x000001DEEA5F0970>>\n"
          ]
        }
      ],
      "source": [
        "print(description_result.as_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZGT9AEDPwca",
        "outputId": "e5cca427-1fb9-4737-9ae5-0b05eb4c9b84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<azure.cognitiveservices.vision.computervision.models._models_py3.ImageCaption at 0x1deea5f0e80>]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description_result.captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0UEv7vQPwca",
        "outputId": "6750c545-ead9-4144-b8f2-b466ac87e585"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5649126768112183"
            ]
          },
          "execution_count": 229,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for caption in description_result.captions:\n",
        "    caption_text = caption.text\n",
        "    caption_confidence = caption.confidence\n",
        "caption_confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg-2BzWSPwca",
        "outputId": "6222cead-a63c-4b9b-d134-019d770f6da8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'a woman holding a box'"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "caption_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE6wV6LtPwca"
      },
      "outputs": [],
      "source": [
        "image_analysis_df.to_csv()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}